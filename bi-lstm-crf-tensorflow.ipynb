{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification Problem\n",
    "\n",
    "We will define a simple sequence classification problem to explore bidirectional LSTMs.\n",
    "\n",
    "The problem is defined as a sequence of random values between 0 and 1. \n",
    "\n",
    "A binary label (0 or 1) is associated with each input. Initially, the output values are all 0. Once the cumulative sum of the input values in the sequence exceeds a threshold, then the output value flips from 0 to 1.\n",
    "\n",
    "A threshold of 1/4 the sequence length is used.\n",
    "\n",
    "For example, below is a sequence of 10 input timesteps (X):\n",
    "\n",
    "```python\n",
    "0.63144003 0.29414551 0.91587952 0.95189228 0.32195638 0.60742236 0.83895793 0.18023048 0.84762691 0.29165514\n",
    "```\n",
    "\n",
    "In this case the threshold is `2.5` and the corresponding classification output (y) would be:\n",
    "\n",
    "```python\n",
    "0 0 0 1 1 1 1 1 1 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as rand\n",
    "from random import random\n",
    "from numpy import array\n",
    "from numpy import cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a sequence classification instance\n",
    "def get_sequence(sequence_length):\n",
    "    # create a sequence of random numbers in [0,1]\n",
    "    X = array([random() for _ in range(sequence_length)])\n",
    "    # calculate cut-off value to change class values\n",
    "    limit = sequence_length / 4.0\n",
    "    # determine the class outcome for each item in cumulative sequence\n",
    "    y = array([0 if x < limit else 1 for x in cumsum(X)])\n",
    "    # reshape input and output data to be suitable for LSTMs\n",
    "    X = X.reshape(1, sequence_length, 1)\n",
    "    y = y.reshape(1, sequence_length, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# create n examples with random sequence lengths between 5 and 15\n",
    "def get_examples(n):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    sequence_length_list = []\n",
    "    for _ in range(n):\n",
    "        sequence_length = rand.randrange(start=5, stop=15)\n",
    "        X, y = get_sequence(sequence_length)\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        sequence_length_list.append(sequence_length)\n",
    "    \n",
    "    return X_list, y_list, sequence_length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.46206663]\n",
      "  [ 0.09480065]\n",
      "  [ 0.9923607 ]\n",
      "  [ 0.20633283]\n",
      "  [ 0.50758544]\n",
      "  [ 0.58507118]\n",
      "  [ 0.3267947 ]]]\n",
      "[[[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [1]]]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, sequence_length_train = get_examples(100)\n",
    "x_test, y_test, sequence_length_test = get_examples(30)\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(sequence_length_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39808336/tensorflow-bidirectional-dynamic-rnn-none-values-error\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn\n",
    "# https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bidirectional lstm + CRF\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 100\n",
    "input_size = 1\n",
    "batch_size  = 32\n",
    "num_units = 128 # the number of units in the LSTM cell\n",
    "number_of_classes = 2 # one-hot encoding\n",
    "\n",
    "input_data   = tf.placeholder(tf.float32, [None, None, input_size], name=\"input_data\")\n",
    "labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\") # shape = (batch, sentence)\n",
    "seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "(output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell, \n",
    "                                                  cell_bw=lstm_bw_cell, \n",
    "                                                  inputs=input_data,\n",
    "                                                  sequence_length=seq_len, \n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "# As we have Bi-LSTM, we have two output, which are not connected. So merge them\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "# fully connected layer\n",
    "W = tf.get_variable(name=\"W\", shape=[2 * num_units, number_of_classes],\n",
    "                dtype=tf.float32)\n",
    "\n",
    "b = tf.get_variable(name=\"b\", shape=[number_of_classes], dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
    "pred = tf.matmul(outputs_flat, W) + b\n",
    "scores = tf.reshape(pred, [-1, seq_len, number_of_classes])\n",
    "\n",
    "# linear-CRF\n",
    "log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(scores, labels, sequence_lengths)\n",
    "\n",
    "loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "# training\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
