{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification Problem\n",
    "\n",
    "We will define a simple sequence classification problem to explore bidirectional LSTMs.\n",
    "\n",
    "The problem is defined as a sequence of random values between 0 and 1. \n",
    "\n",
    "A binary label (0 or 1) is associated with each input. Initially, the output values are all 0. Once the cumulative sum of the input values in the sequence exceeds a threshold, then the output value flips from 0 to 1.\n",
    "\n",
    "A threshold of 1/4 the sequence length is used.\n",
    "\n",
    "For example, below is a sequence of 10 input timesteps (X):\n",
    "\n",
    "```python\n",
    "0.63144003 0.29414551 0.91587952 0.95189228 0.32195638 0.60742236 0.83895793 0.18023048 0.84762691 0.29165514\n",
    "```\n",
    "\n",
    "In this case the threshold is `2.5` and the corresponding classification output (y) would be:\n",
    "\n",
    "```python\n",
    "0 0 0 1 1 1 1 1 1 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as rand\n",
    "from random import random\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a sequence classification instance\n",
    "def get_sequence(sequence_length):\n",
    "    # create a sequence of random numbers in [0,1]\n",
    "    X = np.array([random() for _ in range(sequence_length)])\n",
    "    # calculate cut-off value to change class values\n",
    "    limit = sequence_length / 4.0\n",
    "    # determine the class outcome for each item in cumulative sequence\n",
    "    y = np.array([0 if x < limit else 1 for x in np.cumsum(X)])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# create n examples with random sequence lengths between 5 and 15\n",
    "def get_examples(n):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    sequence_length_list = []\n",
    "    for _ in range(n):\n",
    "        sequence_length = rand.randrange(start=5, stop=15)\n",
    "        X, y = get_sequence(sequence_length)\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        sequence_length_list.append(sequence_length)\n",
    "    \n",
    "    return X_list, y_list, sequence_length_list\n",
    "\n",
    "# Tensorflow requires that all sentences (and all labels) inside the same batch have the same length\n",
    "# so we have to pad the data (and labels) inside the batches with 0's\n",
    "def pad(sentence, max_length):\n",
    "    pad_len = max_length - len(sentence)\n",
    "    padding = np.zeros(pad_len)\n",
    "    return np.concatenate((sentence, padding))\n",
    "    \n",
    "# create batches\n",
    "def batch(data, labels, sequence_lengths, batch_size=2):\n",
    "    n_batch = int(math.ceil(len(data) / batch_size))\n",
    "    index = 0\n",
    "    for _ in range(n_batch):\n",
    "        batch_sequence_lengths = np.array(sequence_lengths[index: index + batch_size])\n",
    "        batch_length = np.array(max(batch_sequence_lengths)) # max length in batch\n",
    "        batch_data = np.array([pad(x, batch_length) for x in data[index: index + batch_size]]) # pad data\n",
    "        batch_labels = np.array([pad(x, batch_length) for x in labels[index: index + batch_size]]) # pad labels\n",
    "        index += batch_size\n",
    "        \n",
    "        # reshape input data to be suitable for LSTMs\n",
    "        batch_data = batch_data.reshape(-1, batch_length, 1)\n",
    "        \n",
    "        yield batch_data, batch_labels, batch_length, batch_sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.73755389]\n",
      "  [ 0.26961524]\n",
      "  [ 0.88026586]\n",
      "  [ 0.9989053 ]\n",
      "  [ 0.00747029]\n",
      "  [ 0.72430263]\n",
      "  [ 0.82384136]\n",
      "  [ 0.56453923]\n",
      "  [ 0.65806141]\n",
      "  [ 0.86632174]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.84252824]\n",
      "  [ 0.84096689]\n",
      "  [ 0.72784385]\n",
      "  [ 0.37255124]\n",
      "  [ 0.73089072]\n",
      "  [ 0.05947006]\n",
      "  [ 0.3028108 ]\n",
      "  [ 0.51689167]\n",
      "  [ 0.30126617]\n",
      "  [ 0.46847721]\n",
      "  [ 0.80309845]\n",
      "  [ 0.31077745]]]\n",
      "[[ 0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.]]\n",
      "12\n",
      "[10 12]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, sequence_length_train = get_examples(100)\n",
    "x_test, y_test, sequence_length_test = get_examples(30)\n",
    "for d, l, max_l, lengths in batch(x_train, y_train, sequence_length_train):\n",
    "    print(d)\n",
    "    print(l)\n",
    "    print(max_l)\n",
    "    print(lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39808336/tensorflow-bidirectional-dynamic-rnn-none-values-error\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn\n",
    "# https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf\n",
    "# https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felipe.zschornack\\AppData\\Local\\Continuum\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# bidirectional lstm + CRF\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 100\n",
    "input_size = 1\n",
    "batch_size  = 32\n",
    "num_units = 128 # the number of units in the LSTM cell\n",
    "number_of_classes = 2\n",
    "\n",
    "input_data   = tf.placeholder(tf.float32, [None, None, input_size], name=\"input_data\")\n",
    "labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\") # shape = (batch, sentence)\n",
    "batch_sequence_length = tf.placeholder(tf.int32)\n",
    "original_sequence_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# scope is mandatory to use LSTMCell\n",
    "with tf.name_scope(\"BiLSTM\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope('backward'):\n",
    "        lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell, \n",
    "                                                                     cell_bw=lstm_bw_cell, \n",
    "                                                                     inputs=input_data,\n",
    "                                                                     sequence_length=original_sequence_lengths, \n",
    "                                                                     dtype=tf.float32,\n",
    "                                                                     scope=\"BiLSTM\")\n",
    "\n",
    "# As we have Bi-LSTM, we have two output, which are not connected. So merge them\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "# fully connected layer\n",
    "W = tf.get_variable(name=\"W\", shape=[2 * num_units, number_of_classes],\n",
    "                dtype=tf.float32)\n",
    "\n",
    "b = tf.get_variable(name=\"b\", shape=[number_of_classes], dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
    "pred = tf.matmul(outputs_flat, W) + b\n",
    "scores = tf.reshape(pred, [-1, batch_sequence_length, number_of_classes])\n",
    "\n",
    "# linear-CRF\n",
    "log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(scores, labels, original_sequence_lengths)\n",
    "\n",
    "loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "# Compute the viterbi sequence and score.\n",
    "viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(scores, transition_params, original_sequence_lengths)\n",
    "\n",
    "# training\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.33%\n",
      "Accuracy: 53.62%\n",
      "Accuracy: 53.97%\n",
      "Accuracy: 56.25%\n",
      "Accuracy: 87.33%\n",
      "Accuracy: 89.13%\n",
      "Accuracy: 91.43%\n",
      "Accuracy: 90.62%\n",
      "Accuracy: 90.67%\n",
      "Accuracy: 92.03%\n",
      "Accuracy: 93.33%\n",
      "Accuracy: 90.62%\n",
      "Accuracy: 91.67%\n",
      "Accuracy: 93.48%\n",
      "Accuracy: 92.06%\n",
      "Accuracy: 96.88%\n",
      "Accuracy: 94.33%\n",
      "Accuracy: 94.20%\n",
      "Accuracy: 94.92%\n",
      "Accuracy: 96.88%\n",
      "Accuracy: 95.33%\n",
      "Accuracy: 94.57%\n",
      "Accuracy: 95.56%\n",
      "Accuracy: 100.00%\n",
      "Accuracy: 96.00%\n",
      "Accuracy: 95.65%\n",
      "Accuracy: 95.87%\n",
      "Accuracy: 100.00%\n",
      "Accuracy: 96.33%\n",
      "Accuracy: 96.38%\n",
      "Accuracy: 96.51%\n",
      "Accuracy: 100.00%\n",
      "Accuracy: 97.33%\n",
      "Accuracy: 96.74%\n",
      "Accuracy: 96.83%\n",
      "Accuracy: 100.00%\n",
      "Accuracy: 97.00%\n",
      "Accuracy: 96.74%\n",
      "Accuracy: 97.46%\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Train for a fixed number of iterations.\n",
    "    for i in range(training_epochs):\n",
    "        for batch_data, batch_labels, batch_seq_len, batch_sequence_lengths in batch(x_train, y_train, sequence_length_train, batch_size):\n",
    "            tf_viterbi_sequence, _ = session.run([viterbi_sequence, train_op], \n",
    "                                                 feed_dict={input_data: batch_data, \n",
    "                                                            labels: batch_labels, \n",
    "                                                            batch_sequence_length: batch_seq_len,\n",
    "                                                            original_sequence_lengths: batch_sequence_lengths })\n",
    "            if i % 10 == 0:\n",
    "                # mask to correct input sizes\n",
    "                mask = (np.expand_dims(np.arange(batch_seq_len), axis=0) <\n",
    "                    np.expand_dims(batch_sequence_lengths, axis=1))\n",
    "                total_labels = np.sum(batch_sequence_lengths)\n",
    "                correct_labels = np.sum((batch_labels == tf_viterbi_sequence) * mask)\n",
    "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "                print(\"Accuracy: %.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Tutorial this code was based on: [Bi-LSTM + CRF with character embeddings for NER and POS](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)\n",
    "2. Bidirectional LSTM in keras (contains the description of the problem used in this code): [How to Develop a Bidirectional LSTM For Sequence Classification in Python with Keras](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/)\n",
    "3. [Example of Bidirectional LSTM implementation in Tensorflow](https://stackoverflow.com/questions/39808336/tensorflow-bidirectional-dynamic-rnn-none-values-error)\n",
    "4. [Example of CRF implementation in Tensorflow](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf)\n",
    "5. Explains in details the LSTM \"num_units\" parameter: [Understanding LSTM in Tensorflow](https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/)\n",
    "6. Explains how variable sequence lengths work in Tensorflow: [Variable Sequence Lengths in TensorFlow](https://danijar.com/variable-sequence-lengths-in-tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
