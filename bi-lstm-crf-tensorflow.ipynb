{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classification Problem\n",
    "\n",
    "We will define a simple sequence classification problem to explore bidirectional LSTMs.\n",
    "\n",
    "The problem is defined as a sequence of random values between 0 and 1. \n",
    "\n",
    "A binary label (0 or 1) is associated with each input. Initially, the output values are all 0. Once the cumulative sum of the input values in the sequence exceeds a threshold, then the output value flips from 0 to 1.\n",
    "\n",
    "A threshold of 1/4 the sequence length is used.\n",
    "\n",
    "For example, below is a sequence of 10 input timesteps (X):\n",
    "\n",
    "```python\n",
    "0.63144003 0.29414551 0.91587952 0.95189228 0.32195638 0.60742236 0.83895793 0.18023048 0.84762691 0.29165514\n",
    "```\n",
    "\n",
    "In this case the threshold is `2.5` and the corresponding classification output (y) would be:\n",
    "\n",
    "```python\n",
    "0 0 0 1 1 1 1 1 1 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as rand\n",
    "from random import random\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a sequence classification instance\n",
    "def get_sequence(sequence_length):\n",
    "    # create a sequence of random numbers in [0,1]\n",
    "    X = np.array([random() for _ in range(sequence_length)])\n",
    "    # calculate cut-off value to change class values\n",
    "    limit = sequence_length / 4.0\n",
    "    # determine the class outcome for each item in cumulative sequence\n",
    "    y = np.array([0 if x < limit else 1 for x in np.cumsum(X)])\n",
    "    # reshape input and output data to be suitable for LSTMs\n",
    "#     X = X.reshape(1, sequence_length, 1)\n",
    "#     y = y.reshape(1, sequence_length, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# create n examples with random sequence lengths between 5 and 15\n",
    "def get_examples(n):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    sequence_length_list = []\n",
    "    for _ in range(n):\n",
    "        sequence_length = rand.randrange(start=5, stop=15)\n",
    "        X, y = get_sequence(sequence_length)\n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        sequence_length_list.append(sequence_length)\n",
    "    \n",
    "    return X_list, y_list, sequence_length_list\n",
    "\n",
    "# Tensorflow requires that all sentences and all labels inside the same batch have the same length\n",
    "# so we have to pad the data inside the batches with 0's\n",
    "def pad(sentence, max_length):\n",
    "    pad_len = max_length - len(sentence)\n",
    "    padding = np.zeros(pad_len)\n",
    "    return np.concatenate((sentence, padding))\n",
    "    \n",
    "# create batches\n",
    "def batch(data, labels, sequence_lengths, batch_size=2):\n",
    "    n_batch = int(math.ceil(len(data) / batch_size))\n",
    "    index = 0\n",
    "    for _ in range(n_batch):\n",
    "        batch_sequence_lengths = np.array(sequence_lengths[index: index + batch_size])\n",
    "        batch_length = np.array(max(batch_sequence_lengths)) # max length in batch\n",
    "        batch_data = np.array([pad(x, batch_length) for x in data[index: index + batch_size]]) # pad data\n",
    "        batch_labels = np.array([pad(x, batch_length) for x in labels[index: index + batch_size]]) # pad labels\n",
    "        index += batch_size\n",
    "        \n",
    "        # reshape\n",
    "        batch_data = batch_data.reshape(-1, batch_length, 1)\n",
    "        yield batch_data, batch_labels, batch_length, batch_sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.08612711]\n",
      "  [ 0.10926359]\n",
      "  [ 0.0990515 ]\n",
      "  [ 0.64913961]\n",
      "  [ 0.8575131 ]\n",
      "  [ 0.45431646]\n",
      "  [ 0.11028445]]\n",
      "\n",
      " [[ 0.07849712]\n",
      "  [ 0.37056293]\n",
      "  [ 0.62773953]\n",
      "  [ 0.89308963]\n",
      "  [ 0.57626275]\n",
      "  [ 0.94557276]\n",
      "  [ 0.        ]]]\n",
      "[[ 0.  0.  0.  0.  1.  1.  1.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.]]\n",
      "7\n",
      "[7 6]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, sequence_length_train = get_examples(100)\n",
    "x_test, y_test, sequence_length_test = get_examples(30)\n",
    "for d, l, max_le, le in batch(x_train, y_train, sequence_length_train):\n",
    "    print(d)\n",
    "    print(l)\n",
    "    print(max_le)\n",
    "    print(le)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39808336/tensorflow-bidirectional-dynamic-rnn-none-values-error\n",
    "# https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn\n",
    "# https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/crf\n",
    "# https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felipe.zschornack\\AppData\\Local\\Continuum\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# bidirectional lstm + CRF\n",
    "learning_rate   = 0.001\n",
    "training_epochs = 100\n",
    "input_size = 1\n",
    "batch_size  = 32\n",
    "num_units = 128 # the number of units in the LSTM cell\n",
    "number_of_classes = 2 # one-hot encoding\n",
    "\n",
    "input_data   = tf.placeholder(tf.float32, [None, None, input_size], name=\"input_data\")\n",
    "labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\") # shape = (batch, sentence)\n",
    "batch_sequence_length = tf.placeholder(tf.int32)\n",
    "original_sequence_lengths = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# scope is mandatory\n",
    "with tf.name_scope(\"BiLSTM\"):\n",
    "    with tf.variable_scope('forward'):\n",
    "        lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    with tf.variable_scope('backward'):\n",
    "        lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell, \n",
    "                                                                     cell_bw=lstm_bw_cell, \n",
    "                                                                     inputs=input_data,\n",
    "                                                                     sequence_length=original_sequence_lengths, \n",
    "                                                                     dtype=tf.float32,\n",
    "                                                                     scope=\"BiLSTM\")\n",
    "\n",
    "# As we have Bi-LSTM, we have two output, which are not connected. So merge them\n",
    "outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "# fully connected layer\n",
    "W = tf.get_variable(name=\"W\", shape=[2 * num_units, number_of_classes],\n",
    "                dtype=tf.float32)\n",
    "\n",
    "b = tf.get_variable(name=\"b\", shape=[number_of_classes], dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer())\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1, 2 * num_units])\n",
    "pred = tf.matmul(outputs_flat, W) + b\n",
    "scores = tf.reshape(pred, [-1, batch_sequence_length, number_of_classes])\n",
    "\n",
    "# linear-CRF\n",
    "log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(scores, labels, original_sequence_lengths)\n",
    "\n",
    "loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "# Compute the viterbi sequence and score.\n",
    "viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(scores, transition_params, original_sequence_lengths)\n",
    "\n",
    "# training\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.17%\n",
      "Accuracy: 54.88%\n",
      "Accuracy: 53.48%\n",
      "Accuracy: 47.62%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Train for a fixed number of iterations.\n",
    "    for i in range(1):#training_epochs):\n",
    "        for batch_data, batch_labels, batch_seq_len, batch_sequence_lengths in batch(x_train, y_train, sequence_length_train, batch_size):\n",
    "            tf_viterbi_sequence, _ = session.run([viterbi_sequence, train_op], \n",
    "                                                 feed_dict={input_data: batch_data, \n",
    "                                                            labels: batch_labels, \n",
    "                                                            batch_sequence_length: batch_seq_len,\n",
    "                                                            original_sequence_lengths: batch_sequence_lengths })\n",
    "            if i % 100 == 0:\n",
    "                # mask to correct input sizes\n",
    "                mask = (np.expand_dims(np.arange(batch_seq_len), axis=0) <\n",
    "                    np.expand_dims(batch_sequence_lengths, axis=1))\n",
    "                total_labels = np.sum(batch_sequence_lengths)\n",
    "                correct_labels = np.sum((batch_labels == tf_viterbi_sequence) * mask)\n",
    "                accuracy = 100.0 * correct_labels / float(total_labels)\n",
    "                print(\"Accuracy: %.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
